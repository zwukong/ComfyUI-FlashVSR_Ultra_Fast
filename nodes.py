#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, gc
import math
import torch
import folder_paths
import comfy.utils
import time
import sys
import psutil

import numpy as np
import torch.nn.functional as F

from einops import rearrange
from huggingface_hub import snapshot_download
try:
    from .src import ModelManager, FlashVSRFullPipeline, FlashVSRTinyPipeline, FlashVSRTinyLongPipeline
    from .src.models.TCDecoder import build_tcdecoder
    from .src.models.utils import clean_vram, get_device_list, Buffer_LQ4x_Proj, Causal_LQ4x_Proj
    from .src.models import wan_video_dit
except ImportError:
    from src import ModelManager, FlashVSRFullPipeline, FlashVSRTinyPipeline, FlashVSRTinyLongPipeline
    from src.models.TCDecoder import build_tcdecoder
    from src.models.utils import clean_vram, get_device_list, Buffer_LQ4x_Proj, Causal_LQ4x_Proj
    from src.models import wan_video_dit

device_choices = get_device_list()

def log(message: str, message_type: str = 'normal', icon: str = ""):
    if icon:
        message = f"{icon} {message}"
        
    if message_type == 'error':
        message = '\033[1;41m' + message + '\033[m'
    elif message_type == 'warning':
        message = '\033[1;31m' + message + '\033[m'
    elif message_type == 'finish':
        message = '\033[1;32m' + message + '\033[m'
    elif message_type == 'info':
        message = '\033[1;33m' + message + '\033[m'
    else:
        message = message
    print(f"{message}", flush=True)

def log_resource_usage(prefix="Resource Usage"):
    ram = psutil.virtual_memory()
    ram_used = ram.used / (1024 ** 3)
    ram_total = ram.total / (1024 ** 3)
    
    msg = f"[{prefix}] RAM: {ram_used:.2f}/{ram_total:.2f} GB"
    
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated() / (1024 ** 3)
        vram_reserved = torch.cuda.max_memory_reserved() / (1024 ** 3) # Using max_memory_reserved as requested
        vram_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        msg += f" | VRAM: {vram_used:.2f}/{vram_reserved:.2f}/{vram_total:.2f} GB (Alloc/MaxRes/Total)"
        
    log(msg, message_type='info', icon="üìä")

def model_download(model_name="JunhaoZhuang/FlashVSR"):
    model_dir = os.path.join(folder_paths.models_dir, model_name.split("/")[-1])
    if not os.path.exists(model_dir):
        log(f"Downloading model '{model_name}' from huggingface...", message_type='info', icon="‚¨áÔ∏è")
        snapshot_download(repo_id=model_name, local_dir=model_dir, local_dir_use_symlinks=False, resume_download=True)

def tensor2video(frames: torch.Tensor):
    # Replaced einops with native PyTorch
    # video_permuted = rearrange(video_squeezed, "C F H W -> F H W C")
    # frames: (1, C, F, H, W) -> squeeze(0) -> (C, F, H, W)
    video_squeezed = frames.squeeze(0)
    video_permuted = video_squeezed.permute(1, 2, 3, 0) # C F H W -> F H W C
    video_final = (video_permuted.float() + 1.0) / 2.0
    return video_final

def largest_8n1_leq(n):  # 8n+1
    return 0 if n < 1 else ((n - 1)//8)*8 + 1

def next_8n5(n):  # next 8n+5
    return 21 if n < 21 else ((n - 5 + 7) // 8) * 8 + 5

def compute_scaled_and_target_dims(w0: int, h0: int, scale: int = 4, multiple: int = 128):
    if w0 <= 0 or h0 <= 0:
        raise ValueError("invalid original size")

    sW, sH = w0 * scale, h0 * scale
    
    # Calculate target dimensions by rounding up to the nearest multiple
    # This ensures we always pad instead of crop, preserving all content
    tW = math.ceil(sW / multiple) * multiple
    tH = math.ceil(sH / multiple) * multiple
    
    return sW, sH, tW, tH

def tensor_upscale_then_center_crop(frame_tensor: torch.Tensor, scale: int, tW: int, tH: int) -> torch.Tensor:
    h0, w0, c = frame_tensor.shape
    tensor_bchw = frame_tensor.permute(2, 0, 1).unsqueeze(0) # HWC -> CHW -> BCHW
    
    sW, sH = w0 * scale, h0 * scale
    upscaled_tensor = F.interpolate(tensor_bchw, size=(sH, sW), mode='bicubic', align_corners=False)
    
    # Lossless padding: if scaled size is different from target, we pad/crop.
    # Since we changed tW/tH to be round UP, sW <= tW and sH <= tH.
    # So we should only need padding.
    
    if sW < tW or sH < tH:
        pad_l = max(0, (tW - sW) // 2)
        pad_r = max(0, tW - sW - pad_l)
        pad_t = max(0, (tH - sH) // 2)
        pad_b = max(0, tH - sH - pad_t)
        upscaled_tensor = F.pad(upscaled_tensor, (pad_l, pad_r, pad_t, pad_b), mode='constant', value=0)
    
    # Just in case tW/tH calculation was different (e.g. smaller), we keep cropping logic for safety
    l = max(0, (upscaled_tensor.shape[3] - tW) // 2)
    t = max(0, (upscaled_tensor.shape[2] - tH) // 2)
    cropped_tensor = upscaled_tensor[:, :, t:t + tH, l:l + tW]

    return cropped_tensor.squeeze(0)

def prepare_input_tensor(image_tensor: torch.Tensor, device, scale: int = 4, dtype=torch.bfloat16):
    N0, h0, w0, _ = image_tensor.shape
    
    multiple = 128 # Keep 128 alignment for VAE/DiT blocks
    sW, sH, tW, tH = compute_scaled_and_target_dims(w0, h0, scale=scale, multiple=multiple)
    num_frames_with_padding = N0 + 4
    F = largest_8n1_leq(num_frames_with_padding)
    
    if F == 0:
        raise RuntimeError(f"Not enough frames after padding. Got {num_frames_with_padding}.")
    
    frames = []
    for i in range(F):
        frame_idx = min(i, N0 - 1)
        frame_slice = image_tensor[frame_idx].to(device)
        tensor_chw = tensor_upscale_then_center_crop(frame_slice, scale=scale, tW=tW, tH=tH).to('cpu').to(dtype) * 2.0 - 1.0
        frames.append(tensor_chw)
        del frame_slice

    vid_stacked = torch.stack(frames, 0)
    vid_final = vid_stacked.permute(1, 0, 2, 3).unsqueeze(0)
    
    del vid_stacked
    clean_vram()
    
    return vid_final, tH, tW, F

def calculate_tile_coords(height, width, tile_size, overlap):
    coords = []
    
    stride = tile_size - overlap
    num_rows = math.ceil((height - overlap) / stride)
    num_cols = math.ceil((width - overlap) / stride)
    
    for r in range(num_rows):
        for c in range(num_cols):
            y1 = r * stride
            x1 = c * stride
            
            y2 = min(y1 + tile_size, height)
            x2 = min(x1 + tile_size, width)
            
            if y2 - y1 < tile_size:
                y1 = max(0, y2 - tile_size)
            if x2 - x1 < tile_size:
                x1 = max(0, x2 - tile_size)
                
            coords.append((x1, y1, x2, y2))
            
    return coords

def create_feather_mask(size, overlap):
    H, W = size
    mask = torch.ones(1, 1, H, W)
    ramp = torch.linspace(0, 1, overlap)
    
    mask[:, :, :, :overlap] = torch.minimum(mask[:, :, :, :overlap], ramp.view(1, 1, 1, -1))
    mask[:, :, :, -overlap:] = torch.minimum(mask[:, :, :, -overlap:], ramp.flip(0).view(1, 1, 1, -1))
    
    mask[:, :, :overlap, :] = torch.minimum(mask[:, :, :overlap, :], ramp.view(1, 1, -1, 1))
    mask[:, :, -overlap:, :] = torch.minimum(mask[:, :, -overlap:, :], ramp.flip(0).view(1, 1, -1, 1))
    
    return mask

def init_pipeline(model, mode, device, dtype, alt_vae="none"):
    model_download(model_name="JunhaoZhuang/"+model)
    model_path = os.path.join(folder_paths.models_dir, model)
    if not os.path.exists(model_path):
        raise RuntimeError(f'Model directory does not exist!\nPlease save all weights to "{model_path}"')
    ckpt_path = os.path.join(model_path, "diffusion_pytorch_model_streaming_dmd.safetensors")
    if not os.path.exists(ckpt_path):
        raise RuntimeError(f'"diffusion_pytorch_model_streaming_dmd.safetensors" does not exist!\nPlease save it to "{model_path}"')
    if alt_vae != "none":
        vae_path = folder_paths.get_full_path_or_raise("vae", alt_vae)
        if not os.path.exists(vae_path):
            raise RuntimeError(f'"{alt_vae}" does not exist!')
    else:
        vae_path = os.path.join(model_path, "Wan2.1_VAE.pth")
        if not os.path.exists(vae_path):
            raise RuntimeError(f'"Wan2.1_VAE.pth" does not exist!\nPlease save it to "{model_path}"')
    lq_path = os.path.join(model_path, "LQ_proj_in.ckpt")
    if not os.path.exists(lq_path):
        raise RuntimeError(f'"LQ_proj_in.ckpt" does not exist!\nPlease save it to "{model_path}"')
    tcd_path = os.path.join(model_path, "TCDecoder.ckpt")
    if not os.path.exists(tcd_path):
        raise RuntimeError(f'"TCDecoder.ckpt" does not exist!\nPlease save it to "{model_path}"')
    current_dir = os.path.dirname(os.path.abspath(__file__))
    prompt_path = os.path.join(current_dir, "posi_prompt.pth")
    
    mm = ModelManager(torch_dtype=dtype, device="cpu")
    if mode == "full":
        mm.load_models([ckpt_path, vae_path])
        pipe = FlashVSRFullPipeline.from_model_manager(mm, device=device)
        pipe.vae.model.encoder = None
        pipe.vae.model.conv1 = None
    else:
        mm.load_models([ckpt_path])
        if mode == "tiny":
            pipe = FlashVSRTinyPipeline.from_model_manager(mm, device=device)
        else:
            pipe = FlashVSRTinyLongPipeline.from_model_manager(mm, device=device)
        multi_scale_channels = [512, 256, 128, 128]
        pipe.TCDecoder = build_tcdecoder(new_channels=multi_scale_channels, device=device, dtype=dtype, new_latent_channels=16+768)
        mis = pipe.TCDecoder.load_state_dict(torch.load(tcd_path, map_location=device), strict=False)
        pipe.TCDecoder.clean_mem()
    
    if model == "FlashVSR":
        pipe.denoising_model().LQ_proj_in = Buffer_LQ4x_Proj(in_dim=3, out_dim=1536, layer_num=1).to(device, dtype=dtype)
    else:
        pipe.denoising_model().LQ_proj_in = Causal_LQ4x_Proj(in_dim=3, out_dim=1536, layer_num=1).to(device, dtype=dtype)
    pipe.denoising_model().LQ_proj_in.load_state_dict(torch.load(lq_path, map_location="cpu"), strict=True)
    pipe.denoising_model().LQ_proj_in.to(device)
    pipe.to(device, dtype=dtype)
    pipe.enable_vram_management(num_persistent_param_in_dit=None)
    pipe.init_cross_kv(prompt_path=prompt_path)
    pipe.load_models_to_device(["dit","vae"])
    pipe.offload_model()

    return pipe

class cqdm:
    def __init__(self, iterable=None, total=None, desc="Processing", enable_debug=False):
        self.desc = desc
        self.pbar = None
        self.iterable = None
        self.total = total
        self.enable_debug = enable_debug
        self.start_time = time.time()
        self.step_idx = 0
        
        if iterable is not None:
            try:
                self.total = len(iterable)
                self.iterable = iter(iterable)
            except TypeError:
                if self.total is None:
                    raise ValueError("Total must be provided for iterables with no length.")

        elif self.total is not None:
            pass
            
        else:
            raise ValueError("Either iterable or total must be provided.")
            
    def __iter__(self):
        if self.iterable is None:
            raise TypeError(f"'{type(self).__name__}' object is not iterable. Did you mean to use it with a 'with' statement?")
        if self.pbar is None:
            self.pbar = comfy.utils.ProgressBar(self.total)
        return self
    
    def __next__(self):
        if self.iterable is None:
            raise TypeError("Cannot call __next__ on a non-iterable cqdm object.")
        try:
            step_start = time.time()
            val = next(self.iterable)
            
            if self.pbar:
                self.pbar.update(1)
            
            self.step_idx += 1
            if self.enable_debug:
                step_end = time.time()
                step_time = step_end - step_start
                # log(f"[{self.desc}] Step {self.step_idx}/{self.total} completed in {step_time:.4f}s", message_type='info', icon="‚è±Ô∏è")
                # More detailed stats can be added here if needed, but logging resource usage every step might be too much spam.
                # However, the user asked for "show each process" and "detailed logging".
                # To avoid excessive spam, we'll log resource usage only if enable_debug is strictly True.
                log_resource_usage(prefix=f"{self.desc} Step {self.step_idx}/{self.total}")

            return val
        except StopIteration:
            total_time = time.time() - self.start_time
            if self.enable_debug:
                log(f"Loop '{self.desc}' finished in {total_time:.2f}s", message_type='finish', icon="‚úÖ")
            raise
            
    def __enter__(self):
        if self.pbar is None:
            self.pbar = comfy.utils.ProgressBar(self.total)
        return self.pbar
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        pass
        
    def __len__(self):
        return self.total

def process_chunk(pipe, frames, scale, color_fix, tiled_vae, tiled_dit, tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio, local_range, seed, force_offload, enable_debug, is_single_frame_input=False):
    """
    Processes a single chunk of frames.
    """
    clean_vram()
    _frames = frames
    _device = pipe.device
    dtype = pipe.torch_dtype
    
    # Padding logic for the chunk
    add = next_8n5(frames.shape[0]) - frames.shape[0]
    padding_frames = frames[-1:, :, :, :].repeat(add, 1, 1, 1)
    _frames = torch.cat([frames, padding_frames], dim=0)

    if tiled_dit:
        N, H, W, C = _frames.shape
        
        final_output_canvas = torch.zeros(
            (N, H * scale, W * scale, C), 
            dtype=torch.float16, 
            device="cpu"
        )
        weight_sum_canvas = torch.zeros_like(final_output_canvas)
        tile_coords = calculate_tile_coords(H, W, tile_size, tile_overlap)
        
        log(f"Starting Tiled Processing: {len(tile_coords)} tiles", message_type='info', icon="üöÄ")
        
        # Instantiate cqdm with enable_debug passed correctly
        for i, (x1, y1, x2, y2) in enumerate(cqdm(tile_coords, desc="Processing Tiles", enable_debug=enable_debug)):
            tile_start = time.time()
            if enable_debug:
                log(f"Processing tile {i+1}/{len(tile_coords)}: ({x1},{y1}) -> ({x2},{y2})", message_type='info', icon="üîÑ")
            
            input_tile = _frames[:, y1:y2, x1:x2, :]
            
            LQ_tile, th, tw, F = prepare_input_tensor(input_tile, _device, scale=scale, dtype=dtype)
            if not isinstance(pipe, FlashVSRTinyLongPipeline):
                LQ_tile = LQ_tile.to(_device)
                
            output_tile_gpu = pipe(
                prompt="", negative_prompt="", cfg_scale=1.0, num_inference_steps=1, seed=seed, tiled=tiled_vae,
                LQ_video=LQ_tile, num_frames=F, height=th, width=tw, is_full_block=False, if_buffer=True,
                topk_ratio=sparse_ratio*768*1280/(th*tw), kv_ratio=kv_ratio, local_range=local_range,
                color_fix=color_fix, unload_dit=unload_dit, force_offload=force_offload,
                enable_debug_logging=enable_debug # Pass debug flag if supported by pipe, though pipe def signature might need update or we handle it via cqdm wrapping inside
            )
            
            processed_tile_cpu = tensor2video(output_tile_gpu).to("cpu")
            
            if enable_debug:
                tile_end = time.time()
                tile_time = tile_end - tile_start
                log(f"Tile {i+1} completed in {tile_time:.2f}s", message_type='info', icon="‚è±Ô∏è")
            
            mask_nchw = create_feather_mask(
                (processed_tile_cpu.shape[1], processed_tile_cpu.shape[2]),
                tile_overlap * scale
            ).to("cpu")
            mask_nhwc = mask_nchw.permute(0, 2, 3, 1)
            out_x1, out_y1 = x1 * scale, y1 * scale
            
            tile_H_scaled = processed_tile_cpu.shape[1]
            tile_W_scaled = processed_tile_cpu.shape[2]
            out_x2, out_y2 = out_x1 + tile_W_scaled, out_y1 + tile_H_scaled
            final_output_canvas[:, out_y1:out_y2, out_x1:out_x2, :] += processed_tile_cpu * mask_nhwc
            weight_sum_canvas[:, out_y1:out_y2, out_x1:out_x2, :] += mask_nhwc
            
            del LQ_tile, output_tile_gpu, processed_tile_cpu, input_tile
            clean_vram()
            
        weight_sum_canvas[weight_sum_canvas == 0] = 1.0
        final_output = final_output_canvas / weight_sum_canvas
    else:
        log("Preparing full frame processing...", message_type='info', icon="üéûÔ∏è")
        if enable_debug:
            log_resource_usage(prefix="Pre-Preprocess")
        
        LQ, th, tw, F = prepare_input_tensor(_frames, _device, scale=scale, dtype=dtype)
        if not isinstance(pipe, FlashVSRTinyLongPipeline):
            LQ = LQ.to(_device)
            
        log(f"Processing {frames.shape[0]} frames...", message_type='info', icon="üöÄ")
        
        process_start = time.time()

        # We need to pass enable_debug to cqdm used inside pipe if possible.
        # But pipe uses `progress_bar_cmd` class or function.
        # We can create a partial or wrapper class.

        class cqdm_debug(cqdm):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs, enable_debug=enable_debug)

        video = pipe(
            prompt="", negative_prompt="", cfg_scale=1.0, num_inference_steps=1, seed=seed, tiled=tiled_vae,
            progress_bar_cmd=cqdm_debug, LQ_video=LQ, num_frames=F, height=th, width=tw, is_full_block=False, if_buffer=True,
            topk_ratio=sparse_ratio*768*1280/(th*tw), kv_ratio=kv_ratio, local_range=local_range,
            color_fix = color_fix, unload_dit=unload_dit, force_offload=force_offload
        )
        process_end = time.time()
        
        if enable_debug:
            log(f"Inference completed in {process_end - process_start:.2f}s", message_type='info', icon="‚è±Ô∏è")
        
        final_output = tensor2video(video).to('cpu')
        
        del video, LQ
        clean_vram()

    if is_single_frame_input and frames.shape[0] == 1:
        # Special handling for single frame if needed, but tensor2video returns [F, H, W, C]
        # logic below seems to handle temporal median if 1 frame? No, wait.
        # If frames.shape[0] == 1, `final_output` is [F_out, H, W, C]. F_out corresponds to padded/processed.
        # The original code did:
        # stacked_image_tensor = torch.median(final_output, dim=0).values.unsqueeze(0).float().to('cpu')
        # This seems to be a way to merge the 8n+1 frames back to 1?
        # Because FlashVSR expands 1 frame to many to process it temporally?
        # If input was 1 frame, padded to 21. Output 21. Median of 21 frames -> 1 frame.
        if frames.shape[0] == 1:
            final_output = final_output.to(_device) # Move back for median calc if it was on CPU? Or keep on CPU?
            # Median on CPU is fine and safer for VRAM
            final_output = final_output.to("cpu")
            stacked_image_tensor = torch.median(final_output, dim=0).values.unsqueeze(0).float()
            del final_output
            clean_vram()
            return stacked_image_tensor

    return final_output[:frames.shape[0], :, :, :]

def flashvsr(pipe, frames, scale, color_fix, tiled_vae, tiled_dit, tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio, local_range, seed, force_offload, enable_debug=False, chunk_size=0):
    clean_vram()
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.ipc_collect()

    start_time = time.time()

    if enable_debug:
        _device = pipe.device
        log(f"Debug Mode: Enabled", message_type='info', icon="üêû")
        log(f"Device: {_device}", message_type='info', icon="üñ•Ô∏è")
        if torch.cuda.is_available():
             log(f"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB", message_type='info', icon="üíæ")
        log(f"Input Frames: {frames.shape}", message_type='info', icon="üéûÔ∏è")
        log(f"Chunk Size: {chunk_size}", message_type='info', icon="üì¶")
        log(f"Tiled DiT: {tiled_dit}, Tiled VAE: {tiled_vae}", message_type='info', icon="üß©")
        log_resource_usage(prefix="Start")

    # Chunking Logic
    total_frames = frames.shape[0]
    final_outputs = []

    is_single_frame_input = (frames.shape[0] == 1)

    if chunk_size > 0 and chunk_size < total_frames:
        num_chunks = math.ceil(total_frames / chunk_size)
        log(f"Splitting video into {num_chunks} chunks (size {chunk_size})...", message_type='info', icon="‚úÇÔ∏è")
        
        for i in range(num_chunks):
            chunk_start = i * chunk_size
            chunk_end = min((i + 1) * chunk_size, total_frames)

            if enable_debug:
                log(f"Processing Chunk {i+1}/{num_chunks}: Frames {chunk_start}-{chunk_end}", message_type='info', icon="üéûÔ∏è")

            chunk_frames = frames[chunk_start:chunk_end]

            chunk_out = process_chunk(
                pipe, chunk_frames, scale, color_fix, tiled_vae, tiled_dit,
                tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio,
                local_range, seed, force_offload, enable_debug,
                is_single_frame_input=is_single_frame_input
            )

            final_outputs.append(chunk_out.cpu()) # Ensure on CPU
            del chunk_out
            clean_vram()

        final_output_tensor = torch.cat(final_outputs, dim=0)
    else:
        final_output_tensor = process_chunk(
            pipe, frames, scale, color_fix, tiled_vae, tiled_dit,
            tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio,
            local_range, seed, force_offload, enable_debug,
            is_single_frame_input=is_single_frame_input
        )

    end_time = time.time()
    total_time = end_time - start_time
    fps = frames.shape[0] / total_time if total_time > 0 else 0
    
    log(f"Done in {total_time:.2f}s ({fps:.2f} FPS).", message_type='finish', icon="‚úÖ")
    
    if torch.cuda.is_available():
        peak_memory = torch.cuda.max_memory_reserved() / 1024**3
        log(f"Peak VRAM used: {peak_memory:.2f} GB", message_type='info', icon="üìà")
        
    log_resource_usage(prefix="Final")
    
    return final_output_tensor


class FlashVSRNodeInitPipe:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (["FlashVSR", "FlashVSR-v1.1"], {
                    "default": "FlashVSR-v1.1",
                    "tooltip": "Model version."
                }),
                "mode": (["tiny", "tiny-long", "full"], {
                    "default": "tiny",
                    "tooltip": 'Using "tiny-long" mode can significantly reduce VRAM used with long video input.'
                }),
                "alt_vae": (["none"] + folder_paths.get_filename_list("vae"), {
                    "default": "none",
                    "tooltip": 'Replaces the built-in VAE, only available in "full" mode.'
                }),
                "force_offload": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Offload all weights to CPU after running a workflow to free up VRAM."
                }),
                "precision": (["fp16", "bf16", "auto"], {
                    "default": "auto",
                    "tooltip": "Data and inference precision. 'auto' selects bf16 if supported, else fp16."
                }),
                "device": (device_choices, {
                    "default": device_choices[0],
                    "tooltip": "Device to load the weights, default: auto (CUDA if available, else CPU)"
                }),
                "attention_mode": (["sparse_sage_attention", "block_sparse_attention", "flash_attention_2", "sdpa"], {
                    "default": "sparse_sage_attention",
                    "tooltip": 'Attention backend selection. "sparse_sage" and "block_sparse" use sparse masks. "flash_attention_2" and "sdpa" use dense attention (potentially slower but higher VRAM usage).'
                }),
            }
        }
    
    RETURN_TYPES = ("PIPE",)
    RETURN_NAMES = ("pipe",)
    FUNCTION = "main"
    CATEGORY = "FlashVSR"
    DESCRIPTION = 'Download the entire "FlashVSR" folder with all the files inside it from "https://huggingface.co/JunhaoZhuang/FlashVSR" and put it in the "ComfyUI/models"'
    
    def main(self, model, mode, alt_vae, force_offload, precision, device, attention_mode):
        _device = device
        if device == "auto":
            _device = "cuda:0" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else device
        if _device == "auto" or _device not in device_choices:
            raise RuntimeError("No devices found to run FlashVSR!")
            
        if _device.startswith("cuda"):
            torch.cuda.set_device(_device)
            # Enforce physical VRAM limit? 
            # torch.cuda.set_per_process_memory_fraction(1.0)
            
        wan_video_dit.ATTENTION_MODE = attention_mode
        
        # Auto bfloat16 detection
        if precision == "auto":
            if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                precision = "bf16"
                log("Auto-detected bf16 support.", message_type='info', icon="‚öôÔ∏è")
            else:
                precision = "fp16"
                log("Defaulting to fp16.", message_type='info', icon="‚öôÔ∏è")
            
        dtype_map = {
            "fp32": torch.float32,
            "fp16": torch.float16,
            "bf16": torch.bfloat16,
        }
        try:
            dtype = dtype_map[precision]
        except:
            dtype = torch.bfloat16
            
        pipe = init_pipeline(model, mode, _device, dtype, alt_vae=alt_vae)
        return((pipe, force_offload),)

class FlashVSRNodeAdv:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "pipe": ("PIPE", {
                    "tooltip": "FlashVSR pipeline"
                }),
                "frames": ("IMAGE", {
                    "tooltip": "Sequential video frames as IMAGE tensor batch"
                }),
                "scale": ("INT", {
                    "default": 2,
                    "min": 2,
                    "max": 4,
                }),
                "color_fix": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Use wavelet transform to correct output video color."
                }),
                "tiled_vae": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Disable tiling: faster decode but higher VRAM usage.\nSet to True for lower memory consumption at the cost of speed."
                }),
                "tiled_dit": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Significantly reduces VRAM usage at the cost of speed."
                }),
                "tile_size": ("INT", {
                    "default": 256,
                    "min": 32,
                    "max": 1024,
                    "step": 32,
                }),
                "tile_overlap": ("INT", {
                    "default": 24,
                    "min": 8,
                    "max": 512,
                    "step": 8,
                }),
                "unload_dit": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Unload DiT before decoding to reduce VRAM peak at the cost of speed."
                }),
                "sparse_ratio": ("FLOAT", {
                    "default": 2.0,
                    "min": 1.5,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "Recommended: 1.5 or 2.0\n1.5 ‚Üí faster; 2.0 ‚Üí more stable"
                }),
                "kv_ratio": ("FLOAT", {
                    "default": 3.0,
                    "min": 1.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "Recommended: 1.0 to 3.0\n1.0 ‚Üí less vram; 3.0 ‚Üí high quality"
                }),
                "local_range": ("INT", {
                    "default": 11,
                    "min": 9,
                    "max": 11,
                    "step": 2,
                    "tooltip": "Recommended: 9 or 11\nlocal_range=9 ‚Üí sharper details; 11 ‚Üí more stable results"
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 1125899906842624
                }),
                "frame_chunk_size": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 10000,
                    "step": 1,
                    "tooltip": "Split processing into chunks of N frames to save VRAM. 0 = Process all frames at once."
                }),
                "enable_debug": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Enable extensive logging for debugging."
                }),
                "keep_models_on_cpu": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Add models to RAM (CPU) after processing, instead of keeping them on VRAM."
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "main"
    CATEGORY = "FlashVSR"
    #DESCRIPTION = ""
    
    def main(self, pipe, frames, scale, color_fix, tiled_vae, tiled_dit, tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio, local_range, seed, frame_chunk_size, enable_debug, keep_models_on_cpu):
        _pipe, _ = pipe
        output = flashvsr(_pipe, frames, scale, color_fix, tiled_vae, tiled_dit, tile_size, tile_overlap, unload_dit, sparse_ratio, kv_ratio, local_range, seed, keep_models_on_cpu, enable_debug, frame_chunk_size)
        return(output,)

class FlashVSRNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "frames": ("IMAGE", {
                    "tooltip": "Sequential video frames as IMAGE tensor batch"
                }),
                "model": (["FlashVSR", "FlashVSR-v1.1"], {
                    "default": "FlashVSR-v1.1",
                    "tooltip": "Model version."
                }),
                "mode": (["tiny", "tiny-long", "full"], {
                    "default": "tiny",
                    "tooltip": 'Using "tiny-long" mode can significantly reduce VRAM used with long video input.'
                }),
                "scale": ("INT", {
                    "default": 2,
                    "min": 2,
                    "max": 4,
                }),
                "tiled_vae": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Disable tiling: faster decode but higher VRAM usage.\nSet to True for lower memory consumption at the cost of speed."
                }),
                "tiled_dit": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Significantly reduces VRAM usage at the cost of speed."
                }),
                "unload_dit": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Unload DiT before decoding to reduce VRAM peak at the cost of speed."
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 1125899906842624
                }),
                "frame_chunk_size": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 10000,
                    "step": 1,
                    "tooltip": "Split processing into chunks of N frames to save VRAM. 0 = Process all frames at once."
                }),
                "attention_mode": (["sparse_sage_attention", "block_sparse_attention", "flash_attention_2", "sdpa"], {
                    "default": "sparse_sage_attention",
                    "tooltip": 'Attention backend selection. "sparse_sage" and "block_sparse" use sparse masks. "flash_attention_2" and "sdpa" use dense attention (potentially slower but higher VRAM usage).'
                }),
                "enable_debug": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Enable extensive logging for debugging."
                }),
                "keep_models_on_cpu": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Add models to RAM (CPU) after processing, instead of keeping them on VRAM."
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "main"
    CATEGORY = "FlashVSR"
    DESCRIPTION = 'Download the entire "FlashVSR" folder with all the files inside it from "https://huggingface.co/JunhaoZhuang/FlashVSR" and put it in the "ComfyUI/models"'
    
    def main(self, model, frames, mode, scale, tiled_vae, tiled_dit, unload_dit, seed, frame_chunk_size, attention_mode, enable_debug, keep_models_on_cpu):
        _device = "cuda:0" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "auto"
        if _device == "auto" or _device not in device_choices:
            raise RuntimeError("No devices found to run FlashVSR!")
            
        if _device.startswith("cuda"):
            torch.cuda.set_device(_device)
            
        wan_video_dit.ATTENTION_MODE = attention_mode
            
        pipe = init_pipeline(model, mode, _device, torch.float16)
        output = flashvsr(pipe, frames, scale, True, tiled_vae, tiled_dit, 256, 24, unload_dit, 2.0, 3.0, 11, seed, keep_models_on_cpu, enable_debug, frame_chunk_size)
        return(output,)

NODE_CLASS_MAPPINGS = {
    "FlashVSRNode": FlashVSRNode,
    "FlashVSRNodeAdv": FlashVSRNodeAdv,
    "FlashVSRInitPipe": FlashVSRNodeInitPipe,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "FlashVSRNode": "FlashVSR Ultra-Fast",
    "FlashVSRNodeAdv": "FlashVSR Ultra-Fast (Advanced)",
    "FlashVSRInitPipe": "FlashVSR Init Pipeline",
}
